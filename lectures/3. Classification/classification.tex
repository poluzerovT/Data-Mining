% presentation
\documentclass{beamer}

\usetheme{Warsaw}

% rus lang
\usepackage[main=russian,english]{babel}

% insert images
\usepackage{wrapfig}

% declare operator
\DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays
\newcommand{\at}[2][]{#1|_{#2}}

% alrorithm
\usepackage{algorithm2e}
\usepackage{algorithm}
\usepackage{algpseudocode}
% \usepackage{algorithmic}

% math
\usepackage{amsmath}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\K}{K}

\newtheorem{rustheorem}{Теорема}

\title[Байесовские методы классификации]{Лекция 3. Байесовские методы классификации}
\subtitle{Основы интеллектуального анализа данных}
\author{Полузёров Т. Д.}
\institute{БГУ ФПМИ}
\date{}

\begin{document}
	
	\begin{frame}
		\titlepage
	\end{frame}
	
	\begin{center}
		\frametitle{Структура лекции}
		\tableofcontents	
	\end{center}
	
	\section{Задача оптимальной классификации}
	
	\subsection{Постановка задачи}
	
	\begin{frame}
		\frametitle{Вероятностная постановка задачи}
		 $\mathbb{X}$ - множество объектов, $\mathbb{Y}$ - множество классов.
		 
		$(\mathbb{X} \times \mathbb{Y})$ - вероятностное пространство с совместной плотностью $p(x, y) = P(y) p(x | y)$
		 
		 \vspace{5pt}
		 
		$P_y := P(y)$ - \textbf{априорные вероятности} классов (prior)
		
		$p_y(x) := p(x | y)$ - \textbf{функции правдоподобия} классов (likelihood)
		
		\vspace{15pt}
		
		Задачи:
		
		\begin{enumerate}
			\item По выборке $(X, Y) \in (\mathbb{X}, \mathbb{Y})$ построить оценки распределений $\hat{P}_y$ и $\hat{p}_y(x)$
			\item По известным распределениям $p_y(x)$ и $P_y$ построить алгоритм $a: \mathbb{X} \rightarrow \mathbb{Y}$ минимизирующий вероятность ошибочной классификации
		\end{enumerate}
	\end{frame}
	
	\begin{frame}
		\frametitle{Функционал среднего риска}
		Алгоритм $a(x)$ разбивает $\mathbb{X}$ на непересекающиеся области $A_y = \{x \in \mathbb{X} | a(x) = y \}$
		
		\vspace{10pt}
		
		Каждой паре $(y, s) \in (\mathbb{Y} \times \mathbb{Y})$ соответствует величина потери $\lambda_{ys}$ при классификации объекта класса $y$ к классу $s$, $\lambda_{yy} = 0$ и $\lambda_{ys} > 0$ при $y \ne s$
		
		\vspace{10pt}
		
		\textbf{Функционал среднего риска}:
		
		 \[
		 R(a) = \sum_{y \in \mathbb{Y}} \sum_{s \in \mathbb{Y}}
		 \lambda_{ys} P_y P(A_s | y)
		 \]
		 
		 где $P(A_s | y) = \int_{A_s} p_y(x) dx$ - вероятность отнесения к классу $s$ объекта класса $y$.	 
	\end{frame}
	
	\subsection{Оптимальный алгоритм}
	
	\begin{frame}
		\frametitle{Минимум среднего риска}
		% теорема
		\begin{rustheorem}[о минимуме среднего риска]
		Если известны априорные вероятности $P_y$ и функции правдоподобия $p_y(x)$, то минимум среднего риска
		\[
		R(a) = \sum_{y \in \mathbb{Y}} \sum_{s \in \mathbb{Y}}
		\lambda_{ys} P_y P(A_s | y)
		\]
		
		достигается алгоритмом
		
		\[
		a(x) = \arg \min_{s \in \mathbb{Y}} \sum_{y \in \mathbb{Y}} \lambda_{ys} P_y p_y(x)
		\]
		\end{rustheorem}
	\end{frame}
	
	\begin{frame}
		\frametitle{Байесовское решающее правило}
		
		\begin{rustheorem}[об оптимальном классификаторе]
			Если известны априорные вероятности $P_y$, функции правдоподобия $p_y(x)$
			\textbf{и ошибка неправильной классификации зависит только от истинного класса}, т.е.
			$\lambda_{ys} = \lambda_{y}, \forall s \ne y$
			, то минимум среднего риска $R(a)$ достигается алгоритмом
			
			\[
			a(x) = \arg \max_{y \in \mathbb{Y}} \lambda_{y} P_y p_y(x)
			\]

			Такой алгоритм называется \textbf{байесовским решающим правилом} (оптимальный байесовский классификатор)
		\end{rustheorem}
		
	\end{frame}
	
	\begin{frame}
		\frametitle{Апостериорные вероятности}
		Вероятность $P(y | x)$ - называется \textbf{апостериорной вероятностью} (posterior).
		
		\vspace{10pt}
		
		Зная $p_y(x)$ и $P_y$, то по формуле Байеса:
		
		\[
		P(y | x) = 
		\frac{p(x, y)}{p(x)} =
		\frac{p_y(x) P_y}{\sum_{s \in \mathbb{Y}} p_s(x) P_s}
		\]
		
		\vspace{15pt}
		
		Величина ожидаемых потерь на объекте $x$:
		
		\[
		R(x) =
		\sum_{y \in \mathbb{Y}} \lambda_{y} P(y | x)
		\]
	\end{frame}
	
	\begin{frame}
		\frametitle{Принцип максимума апостериорной вероятности}
		Альтернативаня запись оптимального байесовскеого классификатора через апостериорные вероятности:
		
		\[
		a(x) = \arg \max_{y \in \mathbb{Y}} \lambda_{y} P(y | x)
		\]
		
		\vspace{15pt}
		
		\begin{itemize}
		\item
		Если классы равнозначны ($\lambda_{y} = \lambda_{s} \forall y, s \in \mathbb{Y}$), то байесовское решающее правило называют \textbf{принципом максимума апостериорной вероятности}.
		
		\item
		В случае равновероятных (сбалансированных) классов ($P_y = \frac{1}{|\mathbb{Y}|}$), объект $x$ просто относится к классу с наибольшим значением плотности $p_y(x)$.
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{Следующая задача оценки плотности}
		Получен оптимальный байесовский классификатор
		\[
		a(x) = \arg \max_{y \in \mathbb{Y}} \lambda_{y} P(y | x)
		\]
		Но в действительности плотности $P(y | x)$ неизвестны. Чтобы построить итоговый классификатор, ставится задача оценить плотность $\hat{P}(y | x)$ по эмпирическим данным $X \in \mathbb{X}, Y \in \mathbb{Y}$.
		
		\vspace{5pt}
		
		\[
		\hat{a}(x) = \arg \max_{y \in \mathbb{Y}} \lambda_{y} \hat{P}(y | x)
		\]
		После замены в байесовском решающем правиле $P(y | x)$ на их оценки $\hat{P}(y | x)$, классификатор перестает быть оптимальным.
	\end{frame}
	
	\section{Задача оценивания плотности}
	
	\begin{frame}
		\frametitle{Восстановление плотности}
		Имеющаяся выборка $(X, Y) = ((x_i, y_i))_{i=1}^{\ell}$ сгенерирована некоторой плотностью $p(x, y)$.
		
		\vspace{5pt}
		
		Оценку совместной плотности можно строить отдельно для $P_y$ и $p_y(x)$, ведь
		$p(x, y) = P_y p_y(x)$
		
		\vspace{15pt}
		
		Оценка $P_y$ строится очень легко:
		\[
		\hat{P}_y = \frac{\ell_y}{\ell},
		\]
		где $\ell_y = |\{(x_i, y_i) \in (X, Y) : y_i = y\}|$
	\end{frame}
		
	\subsection{Наивный подход}
	
	\begin{frame}
		\frametitle{Наивный подход}
		Суть наивного подхода - предположение о независимости признаков между собой.
		Это позволяет упростить
		
		\[
		P(x | y) = \prod_{i=1}^{n} P(x_i | y)
		\]
		
		Для построения итоговой плотности $P(x | y)$ нужно оценить все индивидуальные распределения признаков $P(x_i), i=1, ..., n$.
	\end{frame}
	
	\begin{frame}
		\frametitle{Наивный байесовский классификатор}
		Оценив априорные плотности и индивидуальные функции правдоподобия, получим \textbf{наивный байесовский классификатор}
		
		\[
		a(x) = \arg \max_{y \in \mathbb{Y}} \ln \lambda_{y} \hat{P}(y) + \sum_{i=1}^{n} \ln \hat{P}(x_i | y)
		\]
		
		Предположение о независимости признаков является очень сильным и на практике почти никогда не выполняется. 
	\end{frame}
	
	\subsection{Непараметрическое восстановление плотности}
	
	\begin{frame}
		\frametitle{Одномерный непрерывный случай}
		Пусть $\mathbb{X} = \mathbb{R}$. 
		Эмпирической оценкой плотности есть доля элементов выборки внутри окна шириной $h$
		
		\[
		\hat{p}(x) = \frac{1}{2mh} \sum_{i=1}^{m} [|x - x_i| < h]
		\]
		
		Результат есть кусочно-постоянная функция. Это приводит к появлению зон неуверенности оптимальном классификаторе. Идея состоит в применении ядра
	\end{frame}
	
	\begin{frame}
		\frametitle{Ядерная оценка плотности}
		Функция $K(z)$ называется ядром, если она:
		\begin{itemize}
			\item $K(z) = K(-z)$ - четная
			\item $\int K(z) dz = 1$
			\item $K(z) \ge 0$
		\end{itemize}
		
		\vspace{15pt}
		
		Тогда \textbf{ядерная оценка плотности} имеет вид:
		\[
		\hat{p}_h(x) = \frac{1}{mh} \sum_{i=1}^{m} K\left(\frac{x - x_i}{h} \right)
		\]
		
	\end{frame}
	
	\begin{frame}
		\frametitle{Многомерный случай}
		Ядерная оценка плотности для многомерной величины $X \in \mathbb{R}^n$
		
		\[
		\hat{p}_h(x) = \frac{1}{m} \sum_{i=1}^{m} \prod_{j=1}^{n} \frac{1}{h_j} K \left(\frac{x - x_i}{h_j}\right)
		\]
		
		\vspace{5pt}
		
		Можно обобщить и на случай пространства, где задана функция расстояния между объектами $\rho(z_1, z_2)$ 
		
		 \[
		 \hat{p}_h(x) = \frac{1}{m V(h)} \sum_{i=1}^{m} K \left( \frac{\rho(x, x_i)}{h_j} \right)
		 \]
	\end{frame}
	
	\begin{frame}
		\frametitle{Метод парзеновского окна}
		Применяя ядерную оценку плотности в байесовском решающем правиле, получим \textbf{метод парзеновского окна}
		
		\[
		a(x) = \arg \max_{y \in \mathbb{Y}} \lambda_{y} \sum_{i=1}^{\ell} [y_i = y] K\left(\frac{\rho(x, x_i)}{h} \right)
		\]
	\end{frame}
	
	\subsection{Параметрическое восстановление плотности}
	
	\begin{frame}
		\frametitle{Параметрический подход}
		Имеется выборка $X = (x_1, ..., x_{\ell}) \in \mathbb{X}$. Предполагается, что плотность, порождающая данные, известна \textbf{с точностью до параметра}, $p(x) = \phi(x; \theta)$. Подбор параметров $\theta$ приводится по выборке $X$ с помощью \textbf{метода максимального правдоподобия}.
		
		\vspace{15pt}
		
		\textbf{Нормальный дискриминантный анализ} - случай байесовской классифицакии в предположении о нормальном распределениии всех классов, $p_y(x) \sim N(\mu_y, \sigma_y^2), y \in \mathbb{Y}$.
	\end{frame}
	
	\begin{frame}
		\frametitle{Теорема о разделяющей поверхности}
		\begin{rustheorem}[о форме разделяющей поверхности]
		Если классы имеют $n$-мерные нормальные плотности распределения
		\[
		p_y(x) = N(x; \mu_y, \Sigma_y), y \in \mathbb{Y}
		\]
		то оптимальный баейсовский классификатор задаёт квадратичную разделяющую поверхность.
		Она вырождается в линейную, если ковариационные матрица классов равны $\Sigma_y = \Sigma, y \in \mathbb{Y}$
		\end{rustheorem}
	\end{frame}
	
	\begin{frame}
		\frametitle{Байесовский нормальный классификатор}
		Оценим параметры $\hat{\mu}_y$ и $\hat{\Sigma}_y$ $n$-мерной плотности  по имеющимся данным для каждого класса $y \in \mathbb{Y}$.
		
		\begin{equation*}
			\begin{aligned}[c]
				\hat{\mu}_y = \frac{1}{m} \sum_{i=1}^{m} x_i
			\end{aligned}
			,\qquad
			\begin{aligned}[c]
				\hat{\Sigma}_y = \frac{1}{m} \sum_{i=1}^{m} (x_i - \hat{\mu})(x_i - \hat{\mu})^T
			\end{aligned}
		\end{equation*}
		
		\vspace{5pt}
		
		И воспользуемся идеей оптимального байесовского классификатора
		
		\[
		a(x) = \arg \max_{y \in \mathbb{Y}} \lambda_{y} \hat{P}(y) N(x; \hat{\mu}_y, \hat{\Sigma}_y)
		\]
		
		Такой классификатор называется \textbf{байесовский нормальный классификатор} или \textbf{подстановочный}
	\end{frame}
	
	\begin{frame}
		\frametitle{Линейный дискриминант Фишера}
		Предположив, что ковриационные матрицы классов равны $\Sigma_y = \Sigma, y \in \mathbb{Y}$ и применяя подстановочный алгоритм, получим метод линейного дискриминанта Фишера.
		
		\vspace{15pt}
		
		В таком случае разделяющая поверхность является линейной (в случае нескольких классов - кусочно линейная) и обладает определенными свойствами устойчивости.
	\end{frame}
	
	\section{}
	
	\begin{frame}
		\frametitle{Резюме}
		Байесовский подход к классификации:
		\begin{itemize}
			\item решает более сложную задачу оценивания плотности и только потом производит классификацию
			 \item знание о распределении позволяет строить доверительные интервалы
			 \item  дает понимание вероятностной природы задачи и идеи для некоторых других методов
		\end{itemize}
	\end{frame}
\end{document}